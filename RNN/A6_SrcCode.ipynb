{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# the number of iterations to train for\n",
    "numTrainingIters = 10000\n",
    "\n",
    "# the number of hidden neurons that hold the state of the RNN\n",
    "hiddenUnits = 1000\n",
    "\n",
    "# the number of classes that we are learning over\n",
    "numClasses = 3\n",
    "\n",
    "# the number of data points in a batch\n",
    "batchSize = 100\n",
    "\n",
    "# this function takes a dictionary (called data) which contains \n",
    "# of (dataPointID, (classNumber, matrix)) entries.  Each matrix\n",
    "# is a sequence of vectors; each vector has a one-hot-encoding of\n",
    "# an ascii character, and the sequence of vectors corresponds to\n",
    "# one line of text.  classNumber indicates which file the line of\n",
    "# text came from.  \n",
    "# \n",
    "# The argument maxSeqLen is the maximum length of a line of text\n",
    "# seen so far.  fileName is the name of a file whose contents\n",
    "# we want to add to data.  classNum is an indicator of the class\n",
    "# we are going to associate with text from that file.  linesToUse\n",
    "# tells us how many lines to sample from the file.\n",
    "#\n",
    "# The return val is the new maxSeqLen, as well as the new data\n",
    "# dictionary with the additional lines of text added\n",
    "def addToData (maxSeqLen, data, fileName, classNum, linesToUse):\n",
    "    #\n",
    "    # open the file and read it in\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    #\n",
    "    # sample linesToUse numbers; these will tell us what lines\n",
    "    # from the text file we will use\n",
    "    myInts = np.random.random_integers (0, len(content) - 1, linesToUse)\n",
    "    #\n",
    "    # i is the key of the next line of text to add to the dictionary\n",
    "    i = len(data)\n",
    "    #\n",
    "    # loop thru and add the lines of text to the dictionary\n",
    "    for whichLine in myInts.flat:\n",
    "        #\n",
    "        # get the line and ignore it if it has nothing in it\n",
    "        line = content[whichLine]\n",
    "        if line.isspace () or len(line) == 0:\n",
    "            continue;\n",
    "        #\n",
    "        # take note if this is the longest line we've seen\n",
    "        if len (line) > maxSeqLen:\n",
    "            maxSeqLen = len (line)\n",
    "        #\n",
    "        # create the matrix that will hold this line\n",
    "        temp = np.zeros((len(line), 256))\n",
    "        #\n",
    "        # j is the character we are on\n",
    "        j = 0\n",
    "        # \n",
    "        # loop thru the characters\n",
    "        for ch in line:\n",
    "            #\n",
    "            # non-ascii? ignore\n",
    "            if ord(ch) >= 256:\n",
    "                continue\n",
    "            #\n",
    "            # one hot!\n",
    "            temp[j][ord(ch)] = 1\n",
    "            # \n",
    "            # move onto the next character\n",
    "            j = j + 1\n",
    "            #\n",
    "        # remember the line of text\n",
    "        data[i] = (classNum, temp)\n",
    "        #\n",
    "        # move onto the next line\n",
    "        i = i + 1\n",
    "    #\n",
    "    # and return the dictionary with the new data\n",
    "    return (maxSeqLen, data)\n",
    "\n",
    "# this function takes as input a data set encoded as a dictionary\n",
    "# (same encoding as the last function) and pre-pends every line of\n",
    "# text with empty characters so that each line of text is exactly\n",
    "# maxSeqLen characters in size\n",
    "def pad(maxSeqLen, data):\n",
    "   #\n",
    "   # loop thru every line of text\n",
    "    for i in data:\n",
    "        #\n",
    "        # access the matrix and the label\n",
    "        temp = data[i][1]\n",
    "        label = data[i][0]\n",
    "        # \n",
    "        # get the number of chatacters in this line\n",
    "        len = temp.shape[0]\n",
    "        #\n",
    "        # and then pad so the line is the correct length\n",
    "        padding = np.zeros ((maxSeqLen - len,256)) \n",
    "        data[i] = (label, np.transpose (np.concatenate ((padding, temp), axis = 0)))\n",
    "   #\n",
    "   # return the new data set\n",
    "    return data\n",
    "\n",
    "# this generates a new batch of training data of size batchSize from the\n",
    "# list of lines of text data. This version of generateData is useful for\n",
    "# an RNN because the data set x is a NumPy array with dimensions\n",
    "# [batchSize, 256, maxSeqLen]; it can be unstacked into a series of\n",
    "# matrices containing one-hot character encodings for each data point\n",
    "# using tf.unstack(inputX, axis=2)\n",
    "def generateDataRNN (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.random_integers (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1] for i in myInts.flat)\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# this also generates a new batch of training data, but it represents\n",
    "# the data as a NumPy array with dimensions [batchSize, 256 * maxSeqLen]\n",
    "# where for each data point, all characters have been appended.  Useful\n",
    "# for feed-forward network training\n",
    "def generateDataFeedForward (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.random_integers (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1].flatten () for i in myInts.flat)\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train, test, test_length):\n",
    "    test_add = [data.pop(i) for i in range(len(data)-test_length, len(data))]\n",
    "    train_add = data\n",
    "    # add train data\n",
    "    idx = len(train)\n",
    "    for dp in train_add:\n",
    "        train[idx] = train_add[dp]\n",
    "        idx += 1\n",
    "    # add test data\n",
    "    idx = len(test)\n",
    "    for dp in test_add:\n",
    "        test[idx] = dp\n",
    "        idx += 1\n",
    "        \n",
    "    return (train, test)\n",
    "\n",
    "# create the data dictionary\n",
    "maxSeqLen = 0\n",
    "tr_data = {} # train data\n",
    "te_data = {} # test  data\n",
    "\n",
    "# load up the three data sets into tr_data & te_data\n",
    "data = {}\n",
    "(maxSeqLen1, data) = addToData(maxSeqLen, data, \"Holmes.txt\", 0, 11000)\n",
    "tr_data, te_data = train_test_split(data, tr_data, te_data, 1000)\n",
    "data = {}\n",
    "(maxSeqLen2, data) = addToData(maxSeqLen, data, \"war.txt\", 1, 11000)\n",
    "tr_data, te_data = train_test_split(data, tr_data, te_data, 1000)\n",
    "data = {}\n",
    "(maxSeqLen3, data) = addToData(maxSeqLen, data, \"william.txt\", 2, 11000)\n",
    "tr_data, te_data = train_test_split(data, tr_data, te_data, 1000)\n",
    "\n",
    "# pad each entry in the dictionary with empty characters as needed so\n",
    "# that the sequences are all of the same length\n",
    "maxSeqLen = max(max(maxSeqLen1,maxSeqLen2),maxSeqLen3)\n",
    "tr_data = pad(maxSeqLen, tr_data)\n",
    "te_data = pad(maxSeqLen, te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we build the TensorFlow computation... there are two inputs, \n",
    "# a batch of text lines and a batch of labels\n",
    "inputX = tf.placeholder(tf.float32, [batchSize, 256, maxSeqLen])\n",
    "inputY = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# this is the inital state of the RNN, before processing any data\n",
    "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
    "\n",
    "# the weight matrix that maps the inputs and hidden state to a set of values\n",
    "W = tf.Variable(np.random.normal(0, 0.05, (hiddenUnits + 256, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# biaes for the hidden values\n",
    "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# weights and bias for the final classification\n",
    "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "\n",
    "# unpack the input sequences so that we have a series of matrices,\n",
    "# each of which has a one-hot encoding of the current character from\n",
    "# every input sequence\n",
    "sequenceOfLetters = tf.unstack(inputX, axis=2)\n",
    "\n",
    "\"\"\"\n",
    "Forward Path\n",
    "\"\"\"\n",
    "# now we implement the forward path\n",
    "currentState = initialState\n",
    "for timeTick in sequenceOfLetters:\n",
    "    #\n",
    "    # concatenate the state with the input, then compute the next state\n",
    "    inputPlusState = tf.concat([timeTick, currentState], 1)  \n",
    "    next_state = tf.tanh(tf.matmul(inputPlusState, W) + b) \n",
    "    currentState = next_state\n",
    "\n",
    "# compute the set of outputs\n",
    "outputs = tf.matmul(currentState, W2) + b2\n",
    "\n",
    "predictions = tf.nn.softmax(outputs)\n",
    "\n",
    "# compute the loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
    "totalLoss = tf.reduce_mean(losses)\n",
    "\n",
    "# use gradient descent to train\n",
    "#trainingAlg = tf.train.GradientDescentOptimizer(0.02).minimize(totalLoss)\n",
    "trainingAlg = tf.train.AdagradOptimizer(0.02).minimize(totalLoss)\n",
    "\n",
    "\"\"\"\n",
    "Backward Propagation \n",
    "\"\"\"\n",
    "# and train!!\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    # initialize everything\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #\n",
    "    # and run the training iters\n",
    "    \"\"\"\n",
    "    train the model on tr_data\n",
    "    \"\"\"\n",
    "    for epoch in range(numTrainingIters):\n",
    "        # \n",
    "        # get some data\n",
    "        x, y = generateDataRNN (maxSeqLen, tr_data)\n",
    "        #\n",
    "        # do the training epoch\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _trainingAlg, _currentState, _predictions, _outputs = sess.run(\n",
    "                [totalLoss, trainingAlg, currentState, predictions, outputs],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        #\n",
    "        # just FYI, compute the number of correct predictions\n",
    "        numCorrect = 0\n",
    "        for i in range (len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0\n",
    "            for j in range (numClasses):\n",
    "                if maxVal < _predictions[i][j]:\n",
    "                    maxVal = _predictions[i][j]\n",
    "                    maxPos = j\n",
    "            if maxPos == y[i]:\n",
    "                numCorrect = numCorrect + 1\n",
    "        #\n",
    "        # print out to the screen\n",
    "        print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
    "    \"\"\"\n",
    "    test trained model on te_data\n",
    "    \"\"\"\n",
    "    te_loss = 0\n",
    "    numCorrect = 0\n",
    "    for j in range(0, len(te_data), batchSize):\n",
    "        x = np.stack(te_data[i][1] for i in range(j,j+batchSize))\n",
    "        y = np.stack(np.array((te_data[i][0])) for i in range(j,j+batchSize))\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _predictions = sess.run(\n",
    "                [totalLoss, predictions],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        # update loss\n",
    "        te_loss += _totalLoss/30\n",
    "        # calculate number of correct predictions  \n",
    "        for m in range(len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0 \n",
    "            for n in range(numClasses):\n",
    "                if maxVal < _predictions[m][n]:\n",
    "                    maxVal = _predictions[m][n]\n",
    "                    maxPos = n\n",
    "            if maxPos == y[m]:\n",
    "                numCorrect += 1\n",
    "    print(\"Loss for 3000 randomly chosen document is\", te_loss, \"number correct labels is\",numCorrect,\"out of 3000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenUnits = 500\n",
    "\n",
    "# now we build the TensorFlow computation... there are two inputs, \n",
    "# a batch of text lines and a batch of labels\n",
    "inputX = tf.placeholder(tf.float32, [batchSize, 256, maxSeqLen])\n",
    "inputY = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# this is the inital state of the RNN, before processing any data\n",
    "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
    "\n",
    "# the weight matrix that maps the inputs and hidden state to a set of values\n",
    "W = tf.Variable(np.random.normal(0, 0.05, (2*hiddenUnits + 256, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# biaes for the hidden values\n",
    "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# weights and bias for the final classification\n",
    "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "\n",
    "# unpack the input sequences so that we have a series of matrices,\n",
    "# each of which has a one-hot encoding of the current character from\n",
    "# every input sequence\n",
    "sequenceOfLetters = tf.unstack(inputX, axis=2)\n",
    "\"\"\"\n",
    "Forward Path\n",
    "\"\"\"\n",
    "# now we implement the forward path\n",
    "currentState = initialState\n",
    "TK_history = []\n",
    "for i in range(9):\n",
    "    TK_history.append(initialState)\n",
    "\n",
    "    \n",
    "idx_10_before = 0\n",
    "for timeTick in sequenceOfLetters:\n",
    "    #\n",
    "    # concatenate the state with the input, then compute the next state\n",
    "    ######## time warping #############\n",
    "    TK_history.append(currentState)\n",
    "    state_10_before = TK_history[idx_10_before]\n",
    "    idx_10_before += 1\n",
    "    inputPlusState = tf.concat([timeTick, currentState, state_10_before], 1)\n",
    "    ###################################\n",
    "    next_state = tf.tanh(tf.matmul(inputPlusState, W) + b) \n",
    "    currentState = next_state\n",
    "\n",
    "# compute the set of outputs\n",
    "outputs = tf.matmul(currentState, W2) + b2\n",
    "\n",
    "predictions = tf.nn.softmax(outputs)\n",
    "\n",
    "# compute the loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
    "totalLoss = tf.reduce_mean(losses)\n",
    "\n",
    "# use gradient descent to train\n",
    "#trainingAlg = tf.train.GradientDescentOptimizer(0.02).minimize(totalLoss)\n",
    "trainingAlg = tf.train.AdagradOptimizer(0.02).minimize(totalLoss)\n",
    "\n",
    "\"\"\"\n",
    "Backward Propagation \n",
    "\"\"\"\n",
    "# and train!!\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    # initialize everything\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #\n",
    "    # and run the training iters\n",
    "    for epoch in range(numTrainingIters):\n",
    "        # \n",
    "        # get some data\n",
    "        x, y = generateDataRNN (maxSeqLen, tr_data)\n",
    "        #\n",
    "        # do the training epoch\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _trainingAlg, _currentState, _predictions, _outputs = sess.run(\n",
    "                [totalLoss, trainingAlg, currentState, predictions, outputs],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        #\n",
    "        # just FYI, compute the number of correct predictions\n",
    "        numCorrect = 0\n",
    "        for i in range (len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0\n",
    "            for j in range (numClasses):\n",
    "                if maxVal < _predictions[i][j]:\n",
    "                    maxVal = _predictions[i][j]\n",
    "                    maxPos = j\n",
    "            if maxPos == y[i]:\n",
    "                numCorrect = numCorrect + 1\n",
    "        #\n",
    "        # print out to the screen\n",
    "        print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
    "# Test\n",
    "    te_loss = 0\n",
    "    numCorrect = 0\n",
    "    for j in range(0, len(te_data), batchSize):\n",
    "        x = np.stack(te_data[i][1] for i in range(j,j+batchSize))\n",
    "        y = np.stack(np.array((te_data[i][0])) for i in range(j,j+batchSize))\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _predictions = sess.run(\n",
    "                [totalLoss, predictions],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        # update loss\n",
    "        te_loss += _totalLoss/30\n",
    "        # calculate number of correct predictions  \n",
    "        for m in range(len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0 \n",
    "            for n in range(numClasses):\n",
    "                if maxVal < _predictions[m][n]:\n",
    "                    maxVal = _predictions[m][n]\n",
    "                    maxPos = n\n",
    "            if maxPos == y[m]:\n",
    "                numCorrect += 1\n",
    "    print(\"Loss for 3000 randomly chosen document is\", te_loss, \"number correct labels is\",numCorrect,\"out of 3000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenUnits = 1000\n",
    "\n",
    "# now we build the TensorFlow computation... there are two inputs, \n",
    "# a batch of text lines and a batch of labels\n",
    "inputX = tf.placeholder(tf.float32, [batchSize, 256*maxSeqLen])\n",
    "inputY = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# this is the inital state of the RNN, before processing any data\n",
    "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
    "\n",
    "# the weight matrix that maps the inputs and hidden state to a set of values\n",
    "W = tf.Variable(np.random.normal(0, 0.05, (maxSeqLen*256, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# biaes for the hidden values\n",
    "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# weights and bias for the final classification\n",
    "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "\n",
    "# unpack the input sequences so that we have a series of matrices,\n",
    "# each of which has a one-hot encoding of the current character from\n",
    "# every input sequence\n",
    "# sequenceOfLetters = tf.unstack(inputX, axis=2)\n",
    "\n",
    "\"\"\"\n",
    "Forward Path\n",
    "\"\"\"\n",
    "# now we implement the forward path\n",
    "\"\"\"\n",
    "3-layer fully connected NN\n",
    "\"\"\"\n",
    "currentState = tf.tanh(tf.matmul(inputX, W) + b)\n",
    "outputs = tf.matmul(currentState, W2) + b2\n",
    "predictions = tf.nn.softmax(outputs)\n",
    "\n",
    "# compute the loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
    "totalLoss = tf.reduce_mean(losses)\n",
    "\n",
    "# use gradient descent to train\n",
    "#trainingAlg = tf.train.GradientDescentOptimizer(0.02).minimize(totalLoss)\n",
    "trainingAlg = tf.train.AdagradOptimizer(0.02).minimize(totalLoss)\n",
    "\n",
    "\"\"\"\n",
    "Backward Propagation \n",
    "\"\"\"\n",
    "# and train!!\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    # initialize everything\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #\n",
    "    # and run the training iters\n",
    "    \"\"\"\n",
    "    train the model on tr_data\n",
    "    \"\"\"\n",
    "    for epoch in range(numTrainingIters):\n",
    "        # \n",
    "        # get some data\n",
    "        x, y = generateDataFeedForward(maxSeqLen, tr_data)\n",
    "        #\n",
    "        # do the training epoch\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _trainingAlg, _currentState, _predictions, _outputs = sess.run(\n",
    "                [totalLoss, trainingAlg, currentState, predictions, outputs],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        #\n",
    "        # just FYI, compute the number of correct predictions\n",
    "        numCorrect = 0\n",
    "        for i in range (len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0\n",
    "            for j in range (numClasses):\n",
    "                if maxVal < _predictions[i][j]:\n",
    "                    maxVal = _predictions[i][j]\n",
    "                    maxPos = j\n",
    "            if maxPos == y[i]:\n",
    "                numCorrect = numCorrect + 1\n",
    "        #\n",
    "        # print out to the screen\n",
    "        print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
    "    \"\"\"\n",
    "    test trained model on te_data\n",
    "    \"\"\"\n",
    "    te_loss = 0\n",
    "    numCorrect = 0\n",
    "    x, y = generateDataFeedForward(maxSeqLen, te_data)\n",
    "    _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "    _totalLoss, _predictions = sess.run(\n",
    "            [totalLoss, predictions],\n",
    "            feed_dict={\n",
    "                inputX:x,\n",
    "                inputY:y,\n",
    "                initialState:_currentState\n",
    "            })\n",
    "    # update loss\n",
    "    te_loss += _totalLoss\n",
    "    # calculate number of correct predictions  \n",
    "    for m in range(len(y)):\n",
    "        maxPos = -1\n",
    "        maxVal = 0.0 \n",
    "        for n in range(numClasses):\n",
    "            if maxVal < _predictions[m][n]:\n",
    "                maxVal = _predictions[m][n]\n",
    "                maxPos = n\n",
    "        if maxPos == y[m]:\n",
    "            numCorrect += 1\n",
    "    print(\"Loss for 3000 randomly chosen document is\", te_loss, \"number correct labels is\",numCorrect,\"out of 3000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenUnits = 500\n",
    "\n",
    "# now we build the TensorFlow computation... there are two inputs, \n",
    "# a batch of text lines and a batch of labels\n",
    "inputX = tf.placeholder(tf.float32, [batchSize, 256, maxSeqLen])\n",
    "inputY = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# this is the inital state of the RNN, before processing any data\n",
    "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
    "\n",
    "# the weight matrix that maps the inputs and hidden state to a set of values\n",
    "W = tf.Variable(np.random.normal(0, 0.05, (2*hiddenUnits + 247, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# biaes for the hidden values\n",
    "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# weights and bias for the final classification\n",
    "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "\n",
    "\"\"\"\n",
    "Forward Path\n",
    "\"\"\"\n",
    "# now we implement the forward path\n",
    "currentState = initialState\n",
    "conv_layer = tf.keras.layers.Conv1D(filters = 8, kernel_size=10)(inputX)\n",
    "sequenceOfLetters = tf.unstack(conv_layer, axis = 2)\n",
    "TK_history = []\n",
    "for i in range(9):\n",
    "    TK_history.append(initialState)\n",
    "\n",
    "    \n",
    "idx_10_before = 0\n",
    "for timeTick in sequenceOfLetters:\n",
    "    #\n",
    "    # concatenate the state with the input, then compute the next state\n",
    "    ######## time warping #############\n",
    "    TK_history.append(currentState)\n",
    "    state_10_before = TK_history[idx_10_before]\n",
    "    idx_10_before += 1\n",
    "    inputPlusState = tf.concat([timeTick, currentState, state_10_before], 1)\n",
    "    ###################################\n",
    "    next_state = tf.tanh(tf.matmul(inputPlusState, W) + b) \n",
    "    currentState = next_state\n",
    "\n",
    "# compute the set of outputs\n",
    "outputs = tf.matmul(currentState, W2) + b2\n",
    "\n",
    "predictions = tf.nn.softmax(outputs)\n",
    "\n",
    "# compute the loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
    "totalLoss = tf.reduce_mean(losses)\n",
    "\n",
    "# use gradient descent to train\n",
    "#trainingAlg = tf.train.GradientDescentOptimizer(0.02).minimize(totalLoss)\n",
    "trainingAlg = tf.train.AdagradOptimizer(0.02).minimize(totalLoss)\n",
    "\n",
    "\"\"\"\n",
    "Backward Propagation \n",
    "\"\"\"\n",
    "# and train!!\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    # initialize everything\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #\n",
    "    # and run the training iters\n",
    "    for epoch in range(numTrainingIters):\n",
    "        # \n",
    "        # get some data\n",
    "        x, y = generateDataRNN (maxSeqLen, tr_data)\n",
    "        #\n",
    "        # do the training epoch\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _trainingAlg, _currentState, _predictions, _outputs = sess.run(\n",
    "                [totalLoss, trainingAlg, currentState, predictions, outputs],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        #\n",
    "        # just FYI, compute the number of correct predictions\n",
    "        numCorrect = 0\n",
    "        for i in range (len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0\n",
    "            for j in range (numClasses):\n",
    "                if maxVal < _predictions[i][j]:\n",
    "                    maxVal = _predictions[i][j]\n",
    "                    maxPos = j\n",
    "            if maxPos == y[i]:\n",
    "                numCorrect = numCorrect + 1\n",
    "        #\n",
    "        # print out to the screen\n",
    "        print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
    "# Test\n",
    "    te_loss = 0\n",
    "    numCorrect = 0\n",
    "    for j in range(0, len(te_data), batchSize):\n",
    "        x = np.stack(te_data[i][1] for i in range(j,j+batchSize))\n",
    "        y = np.stack(np.array((te_data[i][0])) for i in range(j,j+batchSize))\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _predictions = sess.run(\n",
    "                [totalLoss, predictions],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        # update loss\n",
    "        te_loss += _totalLoss/30\n",
    "        # calculate number of correct predictions  \n",
    "        for m in range(len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0 \n",
    "            for n in range(numClasses):\n",
    "                if maxVal < _predictions[m][n]:\n",
    "                    maxVal = _predictions[m][n]\n",
    "                    maxPos = n\n",
    "            if maxPos == y[m]:\n",
    "                numCorrect += 1\n",
    "    print(\"Loss for 3000 randomly chosen document is\", te_loss, \"number correct labels is\",numCorrect,\"out of 3000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
